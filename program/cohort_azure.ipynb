{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems That Don't Suck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. [Azure ML Studio Setup](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2)\n",
    "2. [ML Studio Compute creation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-compute-instance?view=azureml-api-2&tabs=python)\n",
    "3. [ML Studio instance pricing](https://azure.microsoft.com/en-us/pricing/details/machine-learning/#pricing)\n",
    "4. [AWS-Azure Compute comparison](https://www.justaftermidnight247.com/insights/aws-to-azure-instance-mapping-for-easy-comparison/)\n",
    "\n",
    "Also see `azure_setup.md`. This file documents initial setup steps to get you up and running\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction and Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:14.438286Z",
     "start_time": "2024-04-17T09:40:14.124287Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ipytest\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# By default, basic information about HTTP sessions (URLs, headers, etc.)\n",
    "# is logged at INFO level. Detailed DEBUG level logging, including request/response\n",
    "# bodies and unredacted headers, can be enabled on a client with the `logging_enable` argument.\n",
    "# See full SDK logging documentation with examples in the link below.\n",
    "# https://learn.microsoft.com/en-us/azure/developer/python/sdk/azure-sdk-logging\n",
    "#\n",
    "# To prevent these from spoiling the output of this notebook cells,\n",
    "# we can change the logging  level to ERROR instead.\n",
    "logging.getLogger(\"azure.ai.ml\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the workspace configuration using the sdk and create a client for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:21.793816Z",
     "start_time": "2024-04-17T09:40:14.441328Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /Users/ayushgupta/Dev/DataspellProjects/ml.school/config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    file_path=\"../config.json\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the pipeline in Local Mode on an ARM64 machine (for example, on Apple Silicon), you will need to use a custom Docker image to train and evaluate the model. Let's create a variable indicating if we are running on an ARM64 machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:22.364770Z",
     "start_time": "2024-04-17T09:40:21.795829Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# We can retrieve the architecture of the local\n",
    "# computer using the `uname -m` command.\n",
    "# architecture = !(uname -m)\n",
    "#\n",
    "# IS_ARM64_ARCHITECTURE = architecture[0] == \"arm64\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "Image builds using ACR tasks will soon be deprecated in favor of serverless computes. The transition should be seamless and automatic so no action is required on your part.\n",
    "You will no longer need to set the imageBuildCompute property on your workspace for network isolation scenarios. All image builds by default will use serverless compute.\n",
    "To learn more about serverless computes visit https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&tabs=python.\n",
    "\n",
    "Known Issues:\n",
    "Serverless Computes require access to the workspace's default storage account.\n",
    "If this storage account is accessible only through an allowed list of subnets, please set the Workspace.ServerlessComputeSubnet to one of the allowed subnets.\n",
    "Please visit https://aka.ms/azureml/environment/secure-training-environments-with-virtual-networks for guidance on how to do this.\n",
    "\n",
    "If you experience any other issues preventing you from building your image, you can temporarily opt out of using serverless computes by setting the image-build-compute property on your workspace to ACR-TASKS-FALLBACK\n",
    "Please visit https://learn.microsoft.com/en-us/cli/azure/ml/workspace?view=azure-cli-latest#az-ml-workspace-update for guidance on how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:23.166232Z",
     "start_time": "2024-04-17T09:40:22.386306Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penguins\n",
      "serverless\n",
      "False\n",
      "2.12\n",
      "py311\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import PipelineJob, PipelineJobSettings\n",
    "\n",
    "# Create a PipelineJobSettings object\n",
    "settings = PipelineJobSettings()\n",
    "\n",
    "# Set the properties\n",
    "settings.default_datastore = \"penguins\"\n",
    "settings.default_compute = \"serverless\"\n",
    "settings.continue_on_step_failure = False\n",
    "settings.framework_version = \"2.12\"\n",
    "settings.py_version = \"py311\"\n",
    "\n",
    "# Create a PipelineJob\n",
    "pipeline_job = PipelineJob(settings=settings)\n",
    "\n",
    "# Now you can inspect the settings of the pipeline_job\n",
    "print(pipeline_job.settings.default_datastore)\n",
    "print(pipeline_job.settings.default_compute)\n",
    "print(pipeline_job.settings.continue_on_step_failure)\n",
    "print(pipeline_job.settings.framework_version)\n",
    "print(pipeline_job.settings.py_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Exploratory Data Analysis\n",
    "\n",
    "Let's run Exploratory Data Analysis on the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data). The goal of this session is to understand the data and the problem we are trying to solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:24.624120Z",
     "start_time": "2024-04-17T09:40:23.170241Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(DATA_FILEPATH)\n",
    "penguins.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:24.907474Z",
     "start_time": "2024-04-17T09:40:24.626127Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "penguins.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the dataset contains the following columns:\n",
    "\n",
    "1. `species`: The species of a penguin. This is the column we want to predict.\n",
    "2. `island`: The island where the penguin was found\n",
    "3. `culmen_length_mm`: The length of the penguin's culmen (bill) in millimeters\n",
    "4. `culmen_depth_mm`: The depth of the penguin's culmen in millimeters\n",
    "5. `flipper_length_mm`: The length of the penguin's flipper in millimeters\n",
    "6. `body_mass_g`: The body mass of the penguin in grams\n",
    "7. `sex`: The sex of the penguin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now display the distribution of values for the èhreee categorical columns in our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:25.189334Z",
     "start_time": "2024-04-17T09:40:24.909489Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "species_distribution = penguins[\"species\"].value_counts()\n",
    "island_distribution = penguins[\"island\"].value_counts()\n",
    "sex_distribution = penguins[\"sex\"].value_counts()\n",
    "\n",
    "print(species_distribution, end=\"\\n\\n\")\n",
    "print(island_distribution, end=\"\\n\\n\")\n",
    "print(sex_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:25.818086Z",
     "start_time": "2024-04-17T09:40:25.198344Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# we will replace that ambiguous character in the sex column with a null value\n",
    "penguins[\"sex\"] = penguins[\"sex\"].replace(\".\", np.nan)\n",
    "sex_distribution = penguins[\"sex\"].value_counts()\n",
    "sex_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:26.420665Z",
     "start_time": "2024-04-17T09:40:25.821095Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# we'll check for missing values in the entire dataset now\n",
    "penguins.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get rid of the missing values. For now, we will replace the missing values with\n",
    "the most frequent value in the column.\n",
    "Later, we’ll use a different strategy to handle missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:28.768717Z",
     "start_time": "2024-04-17T09:40:26.422675Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "penguins = pd.DataFrame(imputer.fit_transform(penguins), columns=penguins.columns)\n",
    "\n",
    "penguins.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:30.360176Z",
     "start_time": "2024-04-17T09:40:28.770728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets visualize the distribution of categorical features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 10))\n",
    "\n",
    "axes[0].bar(species_distribution.index, species_distribution.values)\n",
    "axes[0].set_title(\"Species Distribution\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].bar(island_distribution.index, island_distribution.values)\n",
    "axes[1].set_title(\"Island Distribution\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "axes[2].bar(sex_distribution.index, sex_distribution.values)\n",
    "axes[2].set_title(\"Sex Distribution\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:31.412819Z",
     "start_time": "2024-04-17T09:40:30.362332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize the distribution of numerical features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(penguins[\"culmen_length_mm\"], bins=20)\n",
    "axes[0, 0].set_title(\"Culmen Length Distribution\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[0, 1].hist(penguins[\"culmen_depth_mm\"], bins=20)\n",
    "axes[0, 1].set_title(\"Culmen Depth Distribution\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1, 0].hist(penguins[\"flipper_length_mm\"], bins=20)\n",
    "axes[1, 0].set_title(\"Flipper Length Distribution\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1, 1].hist(penguins[\"body_mass_g\"], bins=20)\n",
    "axes[1, 1].set_title(\"Body Mass Distribution\")\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:32.306602Z",
     "start_time": "2024-04-17T09:40:31.413824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize the relationship between the numerical features and the target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].scatter(penguins[\"culmen_length_mm\"], penguins[\"species\"])\n",
    "axes[0, 0].set_title(\"Culmen Length vs Species\")\n",
    "axes[0, 0].set_xlabel(\"Culmen Length\")\n",
    "\n",
    "axes[0, 1].scatter(penguins[\"culmen_depth_mm\"], penguins[\"species\"])\n",
    "axes[0, 1].set_title(\"Culmen Depth vs Species\")\n",
    "axes[0, 1].set_xlabel(\"Culmen Depth\")\n",
    "\n",
    "axes[1, 0].scatter(penguins[\"flipper_length_mm\"], penguins[\"species\"])\n",
    "axes[1, 0].set_title(\"Flipper Length vs Species\")\n",
    "axes[1, 0].set_xlabel(\"Flipper Length\")\n",
    "\n",
    "axes[1, 1].scatter(penguins[\"body_mass_g\"], penguins[\"species\"])\n",
    "axes[1, 1].set_title(\"Body Mass vs Species\")\n",
    "axes[1, 1].set_xlabel(\"Body Mass\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display the covariance matrix of the dataset. The \"covariance\" measures how changes in one variable are\n",
    "associated with changes in a seconf variable.\n",
    "In other words, the covariance measures the degree to which two variables change together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:32.595868Z",
     "start_time": "2024-04-17T09:40:32.311613Z"
    }
   },
   "outputs": [],
   "source": [
    "penguins.cov(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three examples of what we get from interpreting the covariance matrix below:\n",
    "\n",
    "1. The positive covariance of 50.26 between culmen length and flippler length suggests that larger values of culmen length are associated with larger values of flipper length. As one increases, generally so does the other.\n",
    "2. The positive covariance of 2596.97 between culmen length and body mass suggests that heavier penguins generally have longer culmens. There is a tendency for these two variables to increase together.\n",
    "3. The negative covariance of -742.66 between culmen depth and body mass suggests a general tendency that penguins with deeper culmens weigh less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display the correlation matrix of the dataset. The \"correlation\" measures the strength and direction of a linear relationship between two variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:32.864938Z",
     "start_time": "2024-04-17T09:40:32.597914Z"
    }
   },
   "outputs": [],
   "source": [
    "penguins.corr(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three examples of what we get from interpreting the correlation matrix below:\n",
    "\n",
    "1. Penguins that weight more tend to have longer flippers.\n",
    "2. Penguins with a shallower culmen tend to have longer flippers.\n",
    "3. Penguins with longer culmens tend to have longer flippers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:33.391323Z",
     "start_time": "2024-04-17T09:40:32.866948Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets display the distributino of species by island\n",
    "unique_species = penguins[\"species\"].unique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for species in unique_species:\n",
    "    data = penguins[penguins[\"species\"] == species]\n",
    "    ax.hist(data[\"island\"], bins=5, alpha=0.5, label=species)\n",
    "\n",
    "ax.set_title(\"Species Distribution by Island\")\n",
    "ax.set_xlabel(\"Island\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:34.155326Z",
     "start_time": "2024-04-17T09:40:33.395331Z"
    }
   },
   "outputs": [],
   "source": [
    "# distribution of species by sex\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for species in unique_species:\n",
    "    data = penguins[penguins[\"species\"] == species]\n",
    "    ax.hist(data[\"sex\"], bins=3, alpha=0.5, label=species)\n",
    "\n",
    "ax.set_title(\"Species Distribution by Sex\")\n",
    "ax.set_xlabel(\"Sex\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Splitting and Transforming the Data\n",
    "\n",
    "In this session we'll build a simple\n",
    "[Azure ML Pipeline](https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2)\n",
    "with one step to split and transform the data:\n",
    "\n",
    "<a href=\"images/processing-step.png\" target=\"_blank\"> <img src=\"images/processing-step.png\" alt=\"High-level overview \n",
    "of the Preprocessing Step\" style=\"max-width: 740px;\" /></a>\n",
    "\n",
    "TODO: update the paragraph below with Azure specific details and links\n",
    "\n",
    "We'll use a [Scikit-Learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "for the transformations, and a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [SKLearnProcessor]\n",
    "(https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to\n",
    "execute a preprocessing script.\n",
    "Check the [Azure ML Pipeline Overview](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2) for an introduction to the fundamental\n",
    "components of a Azure ML Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating the Preprocessing Script\n",
    "\n",
    "~~The first step we need in the pipeline is a [Processing Step](https://docs.aws.amazon\n",
    ".com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) to run a script that will split and\n",
    "transform the data.~~\n",
    "\n",
    "~~This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output\n",
    "to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data\n",
    "validation, and model evaluation. Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation for more information.~~\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-inputs-outputs-pipeline?view=azureml-api-2\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-pipeline-component?view=azureml-api-2\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-job-in-pipeline?view=azureml-api-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:34.528101Z",
     "start_time": "2024-04-17T09:40:34.160335Z"
    }
   },
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:34.974087Z",
     "start_time": "2024-04-17T09:40:34.530109Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script_azure.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import tempfile\n",
    "import joblib\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])],\n",
    "    )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(),\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test.species.values).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(\"species\", axis=1)\n",
    "    df_validation = df_validation.drop(\"species\", axis=1)\n",
    "    df_test = df_test.drop(\"species\", axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test)  # noqa: N806\n",
    "\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "\n",
    "    # To compute the data quality baseline, we don't need the\n",
    "    # target variable, so we'll drop it from the dataframe.\n",
    "    df = df.drop(\"species\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "\n",
    "    # We'll use the test baseline to generate predictions later,\n",
    "    # and we can't have a header line because the model won't be\n",
    "    # able to make a prediction for it.\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "        base_directory,\n",
    "        X_train,  # noqa: N803\n",
    "        y_train,\n",
    "        X_validation,  # noqa: N803\n",
    "        y_validation,\n",
    "        X_test,  # noqa: N803\n",
    "        y_test,\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipelines.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines we built\n",
    "    to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                Path(directory) / \"features.joblib\", arcname=\"features.joblib\",\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory='/opt/ml/processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:38.097544Z",
     "start_time": "2024-04-17T09:40:34.976099Z"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -s\n",
    "# | code-fold: true\n",
    "\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(base_directory=directory)\n",
    "\n",
    "    yield directory\n",
    "\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generates_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generates_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def test_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # After transforming the data, the number of features should be 7:\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm = 1\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    number_of_features = 7\n",
    "\n",
    "    # The transformed splits should have an additional column for the target\n",
    "    # variable.\n",
    "    assert train.shape[1] == number_of_features + 1\n",
    "    assert validation.shape[1] == number_of_features + 1\n",
    "    assert test.shape[1] == number_of_features + 1\n",
    "\n",
    "\n",
    "def test_train_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"train-baseline\" / \"train-baseline.csv\",\n",
    "        header=None,\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 0].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_test_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"test-baseline\" / \"test-baseline.csv\", header=None\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 1].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_train_baseline_includes_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n",
    "    assert baseline.columns[0] == \"island\"\n",
    "\n",
    "\n",
    "def test_test_baseline_does_not_include_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n",
    "    assert baseline.columns[0] != \"island\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:40:45.409851Z",
     "start_time": "2024-04-17T09:40:45.094679Z"
    }
   },
   "outputs": [],
   "source": [
    "DEPENDENCY_DIR = Path(\"../container\")\n",
    "sys.path.extend([f\"/{DEPENDENCY_DIR}\"])\n",
    "Path(DEPENDENCY_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T14:34:53.627718Z",
     "start_time": "2024-04-17T14:34:51.677245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name mlschool is registered to workspace, the environment version is 0.1.2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "\n",
    "custom_env_name = \"mlschool\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for penguins pipeline\",\n",
    "    conda_file=os.path.join(DEPENDENCY_DIR, \"ml-dependencies.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.2\",\n",
    ")\n",
    "pipeline_job_env = client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T10:22:14.545171Z",
     "start_time": "2024-04-17T10:22:13.492969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'path': 'azureml://subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourcegroups/ml-school-rg/workspaces/ml-school/datastores/workspaceblobstore/paths/UI/2024-04-08_162623_UTC/penguins.csv', 'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'penguins', 'description': 'penguins dataset in csv format from ml_school repo', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourceGroups/ml-school-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-school/data/penguins/versions/2', 'Resource__source_path': '', 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155e40090>, 'serialize': <msrest.serialization.Serializer object at 0x155eadf50>, 'version': '2', 'latest_version': None, 'datastore': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_asset = client.data.get(\"penguins\", version=\"2\")\n",
    "data_asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T10:56:46.766106Z",
     "start_time": "2024-04-17T10:56:45.953021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureBlobDatastore({'type': <DatastoreType.AZURE_BLOB: 'AzureBlob'>, 'name': 'workspaceblobstore', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourceGroups/ml-school-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-school/datastores/workspaceblobstore', 'Resource__source_path': '', 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x1555c8c90>, 'credentials': {'type': 'account_key'}, 'container_name': 'azureml-blobstore-281c01c8-247f-4d8a-aadf-066659a08b99', 'account_name': 'mlschool8111413849', 'endpoint': 'core.windows.net', 'protocol': 'https'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_store = client.datastores.get_default()\n",
    "data_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:41:01.168011Z",
     "start_time": "2024-04-17T09:41:00.460860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'mlschool', 'description': 'Custom environment for penguins pipeline', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourceGroups/ml-school-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-school/environments/mlschool/versions/0.1.2', 'Resource__source_path': '', 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155dff6d0>, 'serialize': <msrest.serialization.Serializer object at 0x1563ea890>, 'version': '0.1.2', 'conda_file': {'channels': ['defaults', 'conda-forge'], 'dependencies': ['python=3.11', 'numpy', 'pandas', 'scikit-learn', 'tensorflow==2.12.0', 'pip', 'azure-ai-ml', {'pip': ['sagemaker-training']}], 'name': 'ml-dependencies'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"defaults\",\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.11\",\\n    \"numpy\",\\n    \"pandas\",\\n    \"scikit-learn\",\\n    \"tensorflow==2.12.0\",\\n    \"pip\",\\n    \"azure-ai-ml\",\\n    {\\n      \"pip\": [\\n        \"sagemaker-training\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"ml-dependencies\"\\n}'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = client.environments.get(name=\"mlschool\", version=\"0.1.2\")\n",
    "\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T10:37:51.044626Z",
     "start_time": "2024-04-17T10:37:50.809233Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile {CODE_FOLDER}/processing/move_data.py\n",
    "#\n",
    "# import shutil\n",
    "# import argparse\n",
    "#\n",
    "#\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--source', type=str)\n",
    "#     parser.add_argument('--destination', type=str)\n",
    "#     args = parser.parse_args()\n",
    "#\n",
    "#     shutil.copy(src=args.source, dst=args.destination)\n",
    "#\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:26:30.137261Z",
     "start_time": "2024-04-17T11:26:29.898011Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path = (\n",
    "    \"azureml://datastores/workspaceblobstore/paths/opt/processing/input/penguins.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:00:55.814888Z",
     "start_time": "2024-04-17T11:00:55.503937Z"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output, command\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "\n",
    "move_file_component = command(\n",
    "    name=\"move_file\",\n",
    "    inputs={\n",
    "        \"src\": Input(\n",
    "            path=data_asset.path,\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RO_MOUNT,\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"dest\": Output(\n",
    "            path=output_path,\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "    },\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    code=\"./code/processing/move_data.py\",\n",
    "    command=\"python move_data.py --source ${{inputs.src}} --destination ${{outputs.dest}}\",\n",
    "    compute=\"serverless\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T14:50:37.592328Z",
     "start_time": "2024-04-17T14:50:35.193273Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/azure/machine-learning/how-to-read-write-data-v2?view=azureml-api-2&tabs=python\n",
    "\n",
    "from azure.ai.ml import Input, Output, command\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "\n",
    "# ==============================================================\n",
    "# Set the input and output URI paths for the data. Supported paths include:\n",
    "# local: `./<path>\n",
    "# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>\n",
    "# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>\n",
    "# Datastore: azureml://datastores/<data_store_name>/paths/<path>\n",
    "# Data Asset: azureml:<my_data>:<version>\n",
    "# As an example, we set the input path to a file on a public blob container\n",
    "# As an example, we set the output path to a folder in the default datastore\n",
    "# ==============================================================\n",
    "input_path = data_asset.path\n",
    "# when defining output path, you can use your own path but it has to come after the datastores/workspaceblobstore/paths/ prefix\n",
    "output_path = (\n",
    "    \"azureml://datastores/workspaceblobstore/paths/opt/processing/input/penguins.csv\"\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# What type of data are you pointing to?\n",
    "# AssetTypes.URI_FILE (a specific file)\n",
    "# AssetTypes.URI_FOLDER (a folder)\n",
    "# AssetTypes.MLTABLE (a table)\n",
    "# The path we set above is a specific file\n",
    "# ==============================================================\n",
    "data_type = AssetTypes.URI_FILE\n",
    "\n",
    "# ==============================================================\n",
    "# Set the input mode. The most commonly-used modes:\n",
    "# InputOutputModes.RO_MOUNT\n",
    "# InputOutputModes.DOWNLOAD\n",
    "# Set the mode to Read Only (RO) to mount the data\n",
    "# ==============================================================\n",
    "input_mode = InputOutputModes.RO_MOUNT\n",
    "\n",
    "# ==============================================================\n",
    "# Set the output mode. The most commonly-used modes:\n",
    "# InputOutputModes.RW_MOUNT\n",
    "# InputOutputModes.UPLOAD\n",
    "# Set the mode to Read Write (RW) to mount the data\n",
    "# ==============================================================\n",
    "output_mode = InputOutputModes.RW_MOUNT\n",
    "\n",
    "# Set the input and output for the job:\n",
    "inputs = {\"input_data\": Input(type=data_type, path=input_path, mode=input_mode)}\n",
    "\n",
    "outputs = {\n",
    "    \"output_data\": Output(\n",
    "        type=data_type,\n",
    "        path=output_path,\n",
    "        mode=output_mode,\n",
    "        # optional: if you want to create a data asset from the output,\n",
    "        # then uncomment name (name can be set without setting version)\n",
    "        # name = \"<name_of_data_asset>\",\n",
    "        # version = \"<version>\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# This command job copies the data to your default Datastore\n",
    "copy_job = command(\n",
    "    name=\"copy_data\",\n",
    "    description=\"Copy data from one location to another\",\n",
    "    command=\"cp ${{inputs.input_data}} ${{outputs.output_data}}\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    # compute=\"serverless\",\n",
    ")\n",
    "\n",
    "# Submit the command\n",
    "client.jobs.create_or_update(copy_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T14:37:34.216053Z",
     "start_time": "2024-04-17T14:37:31.084791Z"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, command\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "\n",
    "data_asset = client.data.get(\"penguins\", version=\"2\")\n",
    "\n",
    "# to successfully create a job, customize the parameters below based on your workspace resources\n",
    "job = command(\n",
    "    command='ls \"${{inputs.data}}\"',\n",
    "    inputs={\n",
    "        \"data\": Input(\n",
    "            path=data_asset.id,\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RO_MOUNT,\n",
    "        ),\n",
    "    },\n",
    "    environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    ")\n",
    "returned_job = client.jobs.create_or_update(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:27:37.931498Z",
     "start_time": "2024-04-17T11:27:37.671183Z"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "\n",
    "@pipeline()\n",
    "def move_data_pipeline(source: str):\n",
    "    move_data_step = move_file_component(src=source)\n",
    "    print(move_data_step)\n",
    "    return {\"dest\": move_data_step.outputs.dest}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "\n",
    "@pipeline(name=\"Copy Data Pipeline\", settings=settings)\n",
    "def copy_data_pipeline():\n",
    "    copy_data_step = copy_job()\n",
    "    return {\"output_data\": copy_data_step.outputs.output_data}\n",
    "\n",
    "\n",
    "pipeline_job = copy_data_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>Copy Data Pipeline</td><td>neat_bee_y598d349n2</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/neat_bee_y598d349n2?wsid=/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourcegroups/ml-school-rg/workspaces/ml-school&amp;tid=68732935-15d4-4080-89bd-fbbe467d308b\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x1576f5750>}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x1576ee450>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'Copy Data Pipeline', 'is_deterministic': None, 'inputs': {}, 'outputs': {'output_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'copy_data_step': Command({'parameters': {}, 'init': False, 'name': 'copy_data_step', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x1576d99d0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': {'type': 'uri_file', 'path': 'azureml://subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourcegroups/ml-school-rg/workspaces/ml-school/datastores/workspaceblobstore/paths/UI/2024-04-08_162623_UTC/penguins.csv', 'mode': 'ro_mount'}}, 'job_outputs': {'output_data': '${{parent.outputs.output_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x1571ebf10>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x1571e9050>}, 'component': 'azureml_anonymous:0e5b1dd1-2b11-43ca-9feb-b3870cfb3cee', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '36480aef-341e-4f0c-a811-cd2c90faa2c9', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'BUILDER': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'neat_bee_y598d349n2', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/agpt8/ml.school.git', 'mlflow.source.git.branch': 'azure', 'mlflow.source.git.commit': '170653d2dc2e27ce89c0310dd87633b81918cbaa', 'azureml.git.dirty': 'True'}, 'print_as_yaml': False, 'id': '/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourceGroups/ml-school-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-school/jobs/neat_bee_y598d349n2', 'Resource__source_path': '', 'base_path': '/Users/ayushgupta/Dev/DataspellProjects/ml.school/program', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x156d8a110>, 'serialize': <msrest.serialization.Serializer object at 0x1563e8a10>, 'display_name': 'Copy Data Pipeline', 'experiment_name': 'Copy Data Pipeline', 'compute': 'serverless', 'services': {'Tracking': {'endpoint': 'azureml://centralindia.api.azureml.ms/mlflow/v1.0/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourceGroups/ml-school-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-school?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/neat_bee_y598d349n2?wsid=/subscriptions/2acef264-d285-40af-be00-8dae3516307c/resourcegroups/ml-school-rg/workspaces/ml-school&tid=68732935-15d4-4080-89bd-fbbe467d308b', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"Copy Data Pipeline\", compute=\"serverless\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:27:55.156665Z",
     "start_time": "2024-04-17T11:27:41.421862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the source and destination paths\n",
    "source_path = data_asset\n",
    "destination_path = f\"{data_store}/opt/processing/input/\"\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = move_data_pipeline(source=source_path, destination=destination_path)\n",
    "\n",
    "# Submit the pipeline run\n",
    "pipeline_run = client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=\"move_data_experiment\",\n",
    ")\n",
    "\n",
    "# Stream the logs\n",
    "client.jobs.stream(pipeline_run.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.datastores.get_default()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_asset = client.data.get(\"penguins\", version=\"2\")\n",
    "\n",
    "job = command(\n",
    "    name=\"split-trasnform\",\n",
    "    description=\"Split and trasnform data\",\n",
    "    command=\"python script_azure.py\",\n",
    "    inputs={\n",
    "        \"data\": Input(\n",
    "            path=data_asset.path,\n",
    "            type=\"uri_file\",\n",
    "        ),\n",
    "    },\n",
    "    output={\n",
    "        \"train\": Output(\n",
    "            path=\"/opt/ml/processing/train\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "        \"validation\": Output(\n",
    "            path=\"/opt/ml/processing/validation\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "        \"test\": Output(\n",
    "            path=\"/opt/ml/processing/test\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "        \"train_baseline\": Output(\n",
    "            path=\"/opt/ml/processing/train-baseline\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "        \"test_baseline\": Output(\n",
    "            path=\"/opt/ml/processing/test-baseline\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "        \"model\": Output(\n",
    "            path=\"/opt/ml/processing/model\",\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "    },\n",
    "    environment=env,\n",
    "    code=f\"{(CODE_FOLDER / 'processing' / 'script_azure.py').as_posix()}\",\n",
    "    version=\"0.1.2\",\n",
    ")\n",
    "\n",
    "returned_job = client.jobs.create_or_update(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
